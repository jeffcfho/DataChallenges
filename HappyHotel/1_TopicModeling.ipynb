{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeing\n",
    "### Jeff Ho\n",
    "\n",
    "This notebook shows results from topic modeling. I will use LDA to generate topics based on the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:**\n",
    "Provide recommendations to specific hotels\n",
    "\n",
    "**Deliverables:**\n",
    "1. Method to identify topics within reviews\n",
    "2. Method to assign scores for each topic to each hotel\n",
    "\n",
    "**Why?**\n",
    "Understand each hotel’s performance beyond “happy”/“not happy”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Jeff/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models import Phrases\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26521\n",
      "12411\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Is_Response</th>\n",
       "      <th>hotel_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20099</th>\n",
       "      <td>id39751</td>\n",
       "      <td>Was very well located for everything we needed...</td>\n",
       "      <td>happy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8495</th>\n",
       "      <td>id22878</td>\n",
       "      <td>The Carlyle Hotel in DuPont Circle was extreme...</td>\n",
       "      <td>happy</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>id11331</td>\n",
       "      <td>I highly recommend the Andez hotel. My wife an...</td>\n",
       "      <td>happy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8217</th>\n",
       "      <td>id22451</td>\n",
       "      <td>Everything was spectacular. I travel a great d...</td>\n",
       "      <td>happy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15869</th>\n",
       "      <td>id33584</td>\n",
       "      <td>I love the Four Seasons Seattle. The room was ...</td>\n",
       "      <td>happy</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       User_ID                                        Description Is_Response  \\\n",
       "20099  id39751  Was very well located for everything we needed...       happy   \n",
       "8495   id22878  The Carlyle Hotel in DuPont Circle was extreme...       happy   \n",
       "668    id11331  I highly recommend the Andez hotel. My wife an...       happy   \n",
       "8217   id22451  Everything was spectacular. I travel a great d...       happy   \n",
       "15869  id33584  I love the Four Seasons Seattle. The room was ...       happy   \n",
       "\n",
       "       hotel_ID  \n",
       "20099         4  \n",
       "8495          8  \n",
       "668           1  \n",
       "8217          4  \n",
       "15869         8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38932\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "df_happy = pd.read_csv('hotel_happy_reviews.csv')\n",
    "# display(df_happy.head())\n",
    "print(len(df_happy))\n",
    "\n",
    "df_not_happy = pd.read_csv('hotel_not_happy_reviews.csv')\n",
    "# display(df_not_happy.head())\n",
    "print(len(df_not_happy))\n",
    "\n",
    "#join dataframes together\n",
    "df = df_happy.append(df_not_happy,ignore_index=True)\n",
    "display(df.sample(5))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform lemmatizing and pre-processing\n",
    "\n",
    "# Stemming — words are reduced to their root form.\n",
    "# Lemmatizizing — words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Remove stop words\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS:# potentially short words too: and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original review: \n",
      "['My', 'partner', 'and', 'I', 'just', 'completed', 'a', 'two-night', 'stay', 'at', 'the', 'Arctic', 'Club,', 'during', 'our', 'first-ever', 'trip', 'to', 'Seattle.', 'I', \"can't\", 'say', 'enough', 'great', 'things', 'about', 'this', 'property.', 'Not', 'only', 'does', 'it', 'have', 'history', 'in', 'spades', '(it', 'began', 'its', 'life', 'as', 'a', 'turn-of-the-twentieth-century', \"gentleman's\", 'club),', 'but', 'the', 'super-friendly', 'staff', 'and', 'prime', 'Seattle', 'location', 'near', 'historic', 'Pioneer', 'Square', 'make', 'it', 'a', 'must-consider', 'hotel.\\r\\nThe', 'rooms,', 'as', 'noted', 'by', 'other', 'reviewers,', 'are', 'very', 'nicely', 'appointed', 'and', 'pay', 'off', 'the', 'overall', 'historic', 'theme-credibility.', 'We', 'stayed', 'on', 'points,', 'but', 'were', 'upgraded', 'to', 'a', 'corner', 'king', 'with', 'whirlpool.', 'The', 'room', 'overall', 'was', 'very', 'comfortable', '--', 'great', 'bed', 'and', 'bathroom,', 'in', 'particular.\\r\\nIn', 'addition', 'to', 'the', 'points', 'above,', 'the', 'hotel', 'also', 'has', 'a', 'very', 'nice', 'restaurant,', 'Juno,', 'which', 'we', 'visited', 'both', 'mornings', 'for', 'breakfast.\\r\\nThe', 'closest', 'thing', 'to', 'a', 'negative', 'about', 'this', 'property', 'is', 'that', \"it's\", 'slightly', 'off', 'the', 'beaten', 'path', 'in', 'terms', 'of', 'other', 'busier-more', 'happening', 'Seattle', 'neighborhoods', '(around-near', 'Pike', 'Place,', 'for', 'example).', 'But', 'we', 'actually', 'liked', 'that', 'because', 'it', 'contributed', 'to', 'a', 'quiet,', 'relaxing', 'stay.', 'We', 'will', 'definitely', 'be', 'coming', 'back', 'to', 'the', 'Arctic', 'Club', 'on', 'future', 'trips', 'to', 'Seattle', '(oh', 'yeah', '--', 'we', 'totally', 'loved', 'the', 'city,', 'too!).']\n",
      "\n",
      "\n",
      " tokenized and lemmatized review: \n",
      "['partner', 'complet', 'night', 'stay', 'arctic', 'club', 'trip', 'seattl', 'great', 'thing', 'properti', 'histori', 'spade', 'begin', 'life', 'turn', 'twentieth', 'centuri', 'gentleman', 'club', 'super', 'friend', 'staff', 'prime', 'seattl', 'locat', 'near', 'histor', 'pioneer', 'squar', 'consid', 'hotel', 'room', 'note', 'review', 'nice', 'appoint', 'pay', 'overal', 'histor', 'theme', 'credibl', 'stay', 'point', 'upgrad', 'corner', 'king', 'whirlpool', 'room', 'overal', 'comfort', 'great', 'bed', 'bathroom', 'particular', 'addit', 'point', 'hotel', 'nice', 'restaur', 'juno', 'visit', 'morn', 'breakfast', 'closest', 'thing', 'negat', 'properti', 'slight', 'beat', 'path', 'term', 'busier', 'happen', 'seattl', 'neighborhood', 'near', 'pike', 'place', 'exampl', 'actual', 'like', 'contribut', 'quiet', 'relax', 'stay', 'definit', 'come', 'arctic', 'club', 'futur', 'trip', 'seattl', 'oh', 'yeah', 'total', 'love', 'citi']\n"
     ]
    }
   ],
   "source": [
    "# Make sure function works\n",
    "\n",
    "doc_sample = df.loc[4310,'Description']\n",
    "print('original review: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized review: ')\n",
    "print(preprocess(doc_sample))\n",
    "# works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 1.35 s, total: 1min 27s\n",
      "Wall time: 1min 33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [stay, husband, son, way, alaska, cruis, love,...\n",
       "1    [room, nice, clear, updat, recent, clean, bed,...\n",
       "2    [wife, stay, glorious, citi, sf, expens, littl...\n",
       "3    [boyfriend, stay, fairmont, recent, trip, san,...\n",
       "4    [step, time, squar, nice, room, stay, night, g...\n",
       "5    [wife, kid, stay, valentin, weekend, nice, hot...\n",
       "6    [high, recommend, hawthorn, terrac, afford, co...\n",
       "7    [hotel, clean, nice, locat, good, free, shuttl...\n",
       "8    [stay, elan, th, th, octob, like, return, day,...\n",
       "9    [stay, night, happi, locat, min, walk, walk, f...\n",
       "Name: Description, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Pre-process all reviews\n",
    "processed_docs = df['Description'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.4 s, sys: 480 ms, total: 54.9 s\n",
      "Wall time: 55.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Add bigrams and trigrams to docs\n",
    "bigram = Phrases(processed_docs)\n",
    "trigram = Phrases(bigram[processed_docs])\n",
    "for idx in range(len(processed_docs)):\n",
    "    for token in bigram[processed_docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            processed_docs[idx].append(token)\n",
    "    for token in trigram[processed_docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a trigram, add to document.\n",
    "            processed_docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pre-processed reviews.\n",
    "df.join(\n",
    "    pd.DataFrame(processed_docs).rename(columns={'Description':'processed_review'})\n",
    ").to_csv('pre_processed_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 alaska\n",
      "1 alaska_cruis\n",
      "2 ask\n",
      "3 bed\n",
      "4 best\n",
      "5 citi\n",
      "6 cruis\n",
      "7 cruis_ship\n",
      "8 delici\n",
      "9 dinner\n",
      "10 enjoy\n",
      "CPU times: user 5.17 s, sys: 34.4 ms, total: 5.2 s\n",
      "Wall time: 5.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create dictionary from dataset:\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'girlfriend'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many items in the dictionary?\n",
    "print(len(dictionary.items()))\n",
    "dictionary[523]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out extreme values of the dictionary.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 14 (\"great\") appears 2 time.\n",
      "Word 61 (\"nice\") appears 2 time.\n",
      "Word 64 (\"point\") appears 2 time.\n",
      "Word 110 (\"overal\") appears 2 time.\n",
      "Word 135 (\"thing\") appears 2 time.\n",
      "Word 170 (\"trip\") appears 2 time.\n",
      "Word 615 (\"pioneer_squar\") appears 3 time.\n",
      "Word 621 (\"seattl\") appears 4 time.\n",
      "Word 714 (\"club\") appears 3 time.\n",
      "Word 825 (\"properti\") appears 2 time.\n",
      "Word 1116 (\"histor\") appears 2 time.\n",
      "Word 1230 (\"near\") appears 2 time.\n",
      "Word 1828 (\"futur_trip\") appears 3 time.\n",
      "Word 2390 (\"pike_place\") appears 3 time.\n",
      "Word 4699 (\"beat_path\") appears 3 time.\n",
      "Word 6243 (\"oh_yeah\") appears 3 time.\n",
      "CPU times: user 3.36 s, sys: 749 ms, total: 4.11 s\n",
      "Wall time: 4.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create a dictionary reporting how many words and how many times those words appear in each review\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "# bow_corpus[4310]\n",
    "\n",
    "bow_doc_4310 = bow_corpus[4310]\n",
    "# print words that appear more than once\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    if bow_doc_4310[i][1]>1:\n",
    "        print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                         dictionary[bow_doc_4310[i][0]], \n",
    "                                                         bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 841 ms, total: 1min 30s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "# Guess 10 topics for now\n",
    "%time lda_model = gensim.models.LdaModel(bow_corpus, num_topics=10, id2word=dictionary, passes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -1.8576.\n",
      "[([(0.03463333, 'new_york'),\n",
      "   (0.031360663, 'time_squar'),\n",
      "   (0.02025954, 'time'),\n",
      "   (0.016527908, 'holiday_inn'),\n",
      "   (0.015917344, 'locat'),\n",
      "   (0.014613494, 'new'),\n",
      "   (0.012641918, 'squar'),\n",
      "   (0.011588677, 'york'),\n",
      "   (0.010841425, 'great'),\n",
      "   (0.0106599415, 'staff'),\n",
      "   (0.009727528, 'nyc'),\n",
      "   (0.009246683, 'th'),\n",
      "   (0.009007723, 'good'),\n",
      "   (0.008935182, 'central_park'),\n",
      "   (0.008082985, 'clean'),\n",
      "   (0.007236051, 'central'),\n",
      "   (0.006547909, 'night'),\n",
      "   (0.0064682886, 'friend'),\n",
      "   (0.00597356, 'help'),\n",
      "   (0.005810652, 'walk')],\n",
      "  -1.3337978825983157),\n",
      " ([(0.032495447, 'walk'),\n",
      "   (0.030828588, 'walk_distanc'),\n",
      "   (0.019079227, 'locat'),\n",
      "   (0.016126912, 'block_away'),\n",
      "   (0.01357869, 'minut_walk'),\n",
      "   (0.013474562, 'distanc'),\n",
      "   (0.013400719, 'free'),\n",
      "   (0.012215326, 'bed_comfort'),\n",
      "   (0.011115284, 'good'),\n",
      "   (0.010827124, 'block'),\n",
      "   (0.010722507, 'free_internet'),\n",
      "   (0.010494703, 'great'),\n",
      "   (0.0103773875, 'clean'),\n",
      "   (0.009359804, 'restaur'),\n",
      "   (0.008849914, 'away'),\n",
      "   (0.0082629295, 'staff'),\n",
      "   (0.008150609, 'nice'),\n",
      "   (0.008051924, 'place'),\n",
      "   (0.00801952, 'bed'),\n",
      "   (0.00789754, 'internet')],\n",
      "  -1.3895357563122335),\n",
      " ([(0.011254022, 'night'),\n",
      "   (0.01001657, 'bed'),\n",
      "   (0.008939004, 'check'),\n",
      "   (0.008131512, 'desk'),\n",
      "   (0.008016131, 'tell'),\n",
      "   (0.0077240816, 'like'),\n",
      "   (0.007566376, 'get'),\n",
      "   (0.007518736, 'go'),\n",
      "   (0.006669205, 'time'),\n",
      "   (0.006658417, 'ask'),\n",
      "   (0.0062908153, 'day'),\n",
      "   (0.0062146517, 'say'),\n",
      "   (0.005957848, 'bathroom'),\n",
      "   (0.005925883, 'look'),\n",
      "   (0.0057995566, 'door'),\n",
      "   (0.005752813, 'book'),\n",
      "   (0.0054363078, 'come'),\n",
      "   (0.005244494, 'leav'),\n",
      "   (0.005045899, 'call'),\n",
      "   (0.0049088583, 'place')],\n",
      "  -1.400130459160159),\n",
      " ([(0.039250266, 'park'),\n",
      "   (0.014954862, 'car'),\n",
      "   (0.012422232, 'valet_park'),\n",
      "   (0.011134794, 'area'),\n",
      "   (0.011126485, 'night'),\n",
      "   (0.009579247, 'valet'),\n",
      "   (0.009546518, 'nice'),\n",
      "   (0.0089469645, 'place'),\n",
      "   (0.0085646445, 'lot'),\n",
      "   (0.008260659, 'park_garag'),\n",
      "   (0.0076203486, 'pay'),\n",
      "   (0.007545747, 'price'),\n",
      "   (0.0072776712, 'good'),\n",
      "   (0.0070668245, 'clean'),\n",
      "   (0.006898101, 'locat'),\n",
      "   (0.006697549, 'day'),\n",
      "   (0.005950954, 'like'),\n",
      "   (0.005734076, 'look'),\n",
      "   (0.0056506493, 'pool'),\n",
      "   (0.0054413537, 'garag')],\n",
      "  -1.5484380423745814),\n",
      " ([(0.011281508, 'check'),\n",
      "   (0.008592495, 'san_diego'),\n",
      "   (0.0083016595, 'credit_card'),\n",
      "   (0.007961331, 'day'),\n",
      "   (0.007934119, 'night'),\n",
      "   (0.007399111, 'airport'),\n",
      "   (0.007312111, 'servic'),\n",
      "   (0.007170796, 'desk'),\n",
      "   (0.0068969694, 'time'),\n",
      "   (0.0066201678, 'shuttl'),\n",
      "   (0.0064447825, 'arriv'),\n",
      "   (0.006224292, 'staff'),\n",
      "   (0.0062004286, 'charg'),\n",
      "   (0.0058723325, 'go'),\n",
      "   (0.005487541, 'take'),\n",
      "   (0.0053510456, 'minut'),\n",
      "   (0.005269685, 'tell'),\n",
      "   (0.0052017053, 'card'),\n",
      "   (0.0051665325, 'hour'),\n",
      "   (0.004860163, 'wait')],\n",
      "  -1.7289449583725534),\n",
      " ([(0.014865885, 'floor'),\n",
      "   (0.013256316, 'th_floor'),\n",
      "   (0.008842127, 'bathroom'),\n",
      "   (0.008319338, 'good'),\n",
      "   (0.008165695, 'nois'),\n",
      "   (0.00809638, 'bed'),\n",
      "   (0.007657307, 'nice'),\n",
      "   (0.00753725, 'night'),\n",
      "   (0.007493813, 'air_condit'),\n",
      "   (0.0070831566, 'small'),\n",
      "   (0.006859279, 'shower'),\n",
      "   (0.0068117864, 'water'),\n",
      "   (0.006522906, 'locat'),\n",
      "   (0.00648381, 'hot_water'),\n",
      "   (0.00645564, 'air'),\n",
      "   (0.0062012547, 'tv'),\n",
      "   (0.0059227874, 'window'),\n",
      "   (0.005396542, 'area'),\n",
      "   (0.005373658, 'coffe'),\n",
      "   (0.0053691906, 'th')],\n",
      "  -1.777938371285796),\n",
      " ([(0.02607098, 'custom_servic'),\n",
      "   (0.023781408, 'staff'),\n",
      "   (0.02149138, 'servic'),\n",
      "   (0.013592881, 'great'),\n",
      "   (0.013277625, 'busi'),\n",
      "   (0.011845022, 'friend'),\n",
      "   (0.011454106, 'help'),\n",
      "   (0.010209798, 'friend_help'),\n",
      "   (0.010022754, 'locat'),\n",
      "   (0.009846696, 'busi_travel'),\n",
      "   (0.009163332, 'custom'),\n",
      "   (0.00886858, 'busi_trip'),\n",
      "   (0.008662531, 'time'),\n",
      "   (0.008154027, 'clean'),\n",
      "   (0.007607884, 'travel'),\n",
      "   (0.0074624512, 'hampton_inn'),\n",
      "   (0.0070314743, 'good'),\n",
      "   (0.006641767, 'experi'),\n",
      "   (0.0065503693, 'like'),\n",
      "   (0.0064464794, 'comfort')],\n",
      "  -1.9225156274492405),\n",
      " ([(0.018283948, 'year_old'),\n",
      "   (0.01783327, 'suit'),\n",
      "   (0.010741413, 'bed'),\n",
      "   (0.010326925, 'area'),\n",
      "   (0.010153008, 'kid'),\n",
      "   (0.009455842, 'great'),\n",
      "   (0.008664714, 'famili'),\n",
      "   (0.0080513405, 'bedroom'),\n",
      "   (0.0079616895, 'breakfast'),\n",
      "   (0.007911728, 'pool'),\n",
      "   (0.0068728332, 'hilton'),\n",
      "   (0.006279208, 'locat'),\n",
      "   (0.006136037, 'old'),\n",
      "   (0.00609773, 'bedroom_suit'),\n",
      "   (0.0060353805, 'sofa_bed'),\n",
      "   (0.0059647905, 'queen_bed'),\n",
      "   (0.0058543626, 'nice'),\n",
      "   (0.0056496705, 'year'),\n",
      "   (0.0054954267, 'indoor_pool'),\n",
      "   (0.005338235, 'staff')],\n",
      "  -2.311891927517811),\n",
      " ([(0.015585529, 'great'),\n",
      "   (0.011972998, 'staff'),\n",
      "   (0.0107773505, 'servic'),\n",
      "   (0.009860902, 'locat'),\n",
      "   (0.008845231, 'chicago'),\n",
      "   (0.0085918475, 'best_western'),\n",
      "   (0.008367149, 'hot_tub'),\n",
      "   (0.008305608, 'nice'),\n",
      "   (0.008278358, 'bar'),\n",
      "   (0.00779282, 'pool'),\n",
      "   (0.0075759515, 'view'),\n",
      "   (0.0063525178, 'restaur'),\n",
      "   (0.006104943, 'happi_hour'),\n",
      "   (0.0057590143, 'best'),\n",
      "   (0.0056824577, 'friend'),\n",
      "   (0.0055150813, 'love'),\n",
      "   (0.005497044, 'write_review'),\n",
      "   (0.0050246, 'night'),\n",
      "   (0.004845785, 'time'),\n",
      "   (0.0045956625, 'fit_center')],\n",
      "  -2.4006386460713824),\n",
      " ([(0.027605996, 'breakfast'),\n",
      "   (0.020161396, 'san_francisco'),\n",
      "   (0.015049819, 'continent_breakfast'),\n",
      "   (0.014939954, 'good'),\n",
      "   (0.013021894, 'locat'),\n",
      "   (0.012436751, 'union_squar'),\n",
      "   (0.012109073, 'good_valu'),\n",
      "   (0.011566952, 'san'),\n",
      "   (0.010841018, 'breakfast_buffet'),\n",
      "   (0.009646865, 'great'),\n",
      "   (0.009272532, 'clean'),\n",
      "   (0.008775086, 'staff'),\n",
      "   (0.008535131, 'convent_center'),\n",
      "   (0.007800287, 'night'),\n",
      "   (0.0076999315, 'embassi_suit'),\n",
      "   (0.007309327, 'san_antonio'),\n",
      "   (0.0071664806, 'nice'),\n",
      "   (0.007064846, 'free'),\n",
      "   (0.0070238514, 'fisherman_wharf'),\n",
      "   (0.0067567267, 'francisco')],\n",
      "  -2.7616848715728715)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = lda_model.top_topics(bow_corpus)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / 10\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save('lda.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"great\" + 0.012*\"staff\" + 0.011*\"servic\" + 0.010*\"locat\" + 0.009*\"chicago\"'),\n",
       " (1,\n",
       "  '0.035*\"new_york\" + 0.031*\"time_squar\" + 0.020*\"time\" + 0.017*\"holiday_inn\" + 0.016*\"locat\"'),\n",
       " (2,\n",
       "  '0.039*\"park\" + 0.015*\"car\" + 0.012*\"valet_park\" + 0.011*\"area\" + 0.011*\"night\"'),\n",
       " (3,\n",
       "  '0.011*\"check\" + 0.009*\"san_diego\" + 0.008*\"credit_card\" + 0.008*\"day\" + 0.008*\"night\"'),\n",
       " (4,\n",
       "  '0.026*\"custom_servic\" + 0.024*\"staff\" + 0.021*\"servic\" + 0.014*\"great\" + 0.013*\"busi\"'),\n",
       " (5,\n",
       "  '0.011*\"night\" + 0.010*\"bed\" + 0.009*\"check\" + 0.008*\"desk\" + 0.008*\"tell\"'),\n",
       " (6,\n",
       "  '0.028*\"breakfast\" + 0.020*\"san_francisco\" + 0.015*\"continent_breakfast\" + 0.015*\"good\" + 0.013*\"locat\"'),\n",
       " (7,\n",
       "  '0.018*\"year_old\" + 0.018*\"suit\" + 0.011*\"bed\" + 0.010*\"area\" + 0.010*\"kid\"'),\n",
       " (8,\n",
       "  '0.032*\"walk\" + 0.031*\"walk_distanc\" + 0.019*\"locat\" + 0.016*\"block_away\" + 0.014*\"minut_walk\"'),\n",
       " (9,\n",
       "  '0.015*\"floor\" + 0.013*\"th_floor\" + 0.009*\"bathroom\" + 0.008*\"good\" + 0.008*\"nois\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_copy = gensim.models.LdaModel.load('lda.model')\n",
    "lda_copy.show_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from topic modeling\n",
    "Many of the topics seem to make sense (e.g., 2: parking, 4: service, 6: breakfast, 7: kid, 8: distance). Others seem to be focused on geography (e.g., 1: new york), which could be because of the imbalanced data (e.g., too many reviews for the NY hotel.\n",
    "\n",
    "**Next steps:** Get topic scores for each review / hotel?\n",
    "\n",
    "**Validation:** Look at strongest reviews for each topic to interpret. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get topic scores for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.23514672), (4, 0.28776655), (7, 0.10101567), (8, 0.37018186)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_document_topics = [lda_model.get_document_topics(review) for review in bow_corpus]\n",
    "display(get_document_topics[4310])\n",
    "# This review is mostly topic 8, but also some parts 3 and 4 (and weaky 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps off Times Square, nice rooms, stayed - nights, great for a short visit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 0.9249818)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Did the assignment work? Let's check this one....\n",
    "print(df.loc[4,'Description'])\n",
    "display(get_document_topics[4])\n",
    "#Topic 1 is the one about NY and Times Square, so yes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.614366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>0.072890</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146320</td>\n",
       "      <td>0.167900</td>\n",
       "      <td>0.294863</td>\n",
       "      <td>0.032901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141913</td>\n",
       "      <td>0.210382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.230113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.710055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.748277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.235713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.924982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.119976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109693</td>\n",
       "      <td>0.519192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012953</td>\n",
       "      <td>0.016369</td>\n",
       "      <td>0.218998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.157669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288282</td>\n",
       "      <td>0.162744</td>\n",
       "      <td>0.130608</td>\n",
       "      <td>0.150564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121528</td>\n",
       "      <td>0.135007</td>\n",
       "      <td>0.113633</td>\n",
       "      <td>0.623513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38932 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.614366  0.000000  0.000000  0.259785  0.000000  0.000000  0.000000   \n",
       "0   0.000000  0.146320  0.167900  0.294863  0.032901  0.000000  0.000000   \n",
       "0   0.230113  0.000000  0.051075  0.000000  0.000000  0.000000  0.710055   \n",
       "0   0.000000  0.000000  0.000000  0.000000  0.748277  0.000000  0.235713   \n",
       "0   0.000000  0.924982  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "0   0.119976  0.000000  0.109693  0.519192  0.000000  0.000000  0.000000   \n",
       "0   0.157669  0.000000  0.000000  0.288282  0.162744  0.130608  0.150564   \n",
       "0   0.000000  0.000000  0.623791  0.000000  0.000000  0.000000  0.357593   \n",
       "0   0.000000  0.000000  0.121528  0.135007  0.113633  0.623513  0.000000   \n",
       "0   0.000000  0.000000  0.293614  0.000000  0.000000  0.424858  0.000000   \n",
       "\n",
       "           7         8         9  \n",
       "0   0.042424  0.072890  0.000000  \n",
       "0   0.000000  0.141913  0.210382  \n",
       "0   0.000000  0.000000  0.000000  \n",
       "0   0.000000  0.000000  0.000000  \n",
       "0   0.000000  0.000000  0.000000  \n",
       "..       ...       ...       ...  \n",
       "0   0.012953  0.016369  0.218998  \n",
       "0   0.000000  0.000000  0.097118  \n",
       "0   0.000000  0.000000  0.000000  \n",
       "0   0.000000  0.000000  0.000000  \n",
       "0   0.196522  0.000000  0.071660  \n",
       "\n",
       "[38932 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Get columns representing topic affinities for assigning scores\n",
    "ts = get_document_topics\n",
    "review_topics = pd.DataFrame(columns=range(10))\n",
    "for t in ts:\n",
    "    dummy_dict = {}\n",
    "    for tup in t:\n",
    "        dummy_dict[tup[0]] = [tup[1]]\n",
    "#     print(dummy_dict)\n",
    "#     display(pd.DataFrame.from_dict(dummy_dict))\n",
    "    review_topics = review_topics.append(pd.DataFrame.from_dict(dummy_dict))\n",
    "display(review_topics.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write review topics to a csv\n",
    "review_topics.to_csv('review_topic_probabilities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
